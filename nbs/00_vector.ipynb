{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client\n",
    "\n",
    "> A module for writing and querying vectors to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "service_url = os.environ['TIMESCALE_SERVICE_URL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncpg\n",
    "import uuid\n",
    "from pgvector.asyncpg import register_vector\n",
    "from typing import (List, Optional, Union, Dict, Tuple, Any, Iterable)\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "SEARCH_RESULT_ID_IDX = 0\n",
    "SEARCH_RESULT_METADATA_IDX = 1\n",
    "SEARCH_RESULT_CONTENTS_IDX = 2\n",
    "SEARCH_RESULT_EMBEDDING_IDX = 3\n",
    "SEARCH_RESULT_DISTANCE_IDX = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Predicates:\n",
    "    logical_operators = {\n",
    "        \"AND\": \"AND\",\n",
    "        \"OR\": \"OR\",\n",
    "        \"NOT\": \"NOT\",\n",
    "    }\n",
    "\n",
    "    operators_mapping = {\n",
    "        \"=\": \"=\",\n",
    "        \"==\": \"=\",\n",
    "        \">=\": \">=\",\n",
    "        \">\": \">\",\n",
    "        \"<=\": \"<=\",\n",
    "        \"<\": \"<\",\n",
    "        \"!=\": \"<>\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, *clauses: Union['Predicates', Tuple[str, str], Tuple[str, str, str]], operator: str = 'AND'):\n",
    "        \"\"\"\n",
    "        Predicates class defines predicates on the object metadata. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clauses\n",
    "            Predicate clauses. Can be either another Predicates object or a tuple of the form (field, operator, value) or (field, value).\n",
    "        Operator\n",
    "            Logical operator to use when combining the clauses. Can be one of 'AND', 'OR', 'NOT'. Defaults to 'AND'.\n",
    "        \"\"\"\n",
    "        if operator not in self.logical_operators: \n",
    "            raise ValueError(f\"invalid operator: {operator}\")\n",
    "        self.operator = operator\n",
    "        self.clauses = list(clauses)\n",
    "\n",
    "    def add_clause(self, *clause: Union['Predicates', Tuple[str, str], Tuple[str, str, str]]):\n",
    "        \"\"\"\n",
    "        Add a clause to the predicates object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clause: 'Predicates' or Tuple[str, str] or Tuple[str, str, str]\n",
    "            Predicate clause. Can be either another Predicates object or a tuple of the form (field, operator, value) or (field, value).\n",
    "        \"\"\"\n",
    "        self.clauses.extend(list(clause))\n",
    "        \n",
    "    def __and__(self, other):\n",
    "        new_predicates = Predicates(self, other, operator='AND')\n",
    "        return new_predicates\n",
    "\n",
    "    def __or__(self, other):\n",
    "        new_predicates = Predicates(self, other, operator='OR')\n",
    "        return new_predicates\n",
    "\n",
    "    def __invert__(self):\n",
    "        new_predicates = Predicates(self, operator='NOT')\n",
    "        return new_predicates\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Predicates):\n",
    "            return False\n",
    "\n",
    "        return (\n",
    "            self.operator == other.operator and\n",
    "            self.clauses == other.clauses\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.operator:\n",
    "            return f\"{self.operator}({', '.join(repr(clause) for clause in self.clauses)})\"\n",
    "        else:\n",
    "            return repr(self.clauses)\n",
    "\n",
    "    def build_query(self, params: List) -> Tuple[str, List]:\n",
    "        \"\"\"\n",
    "        Build the SQL query string and parameters for the predicates object.\n",
    "        \"\"\"\n",
    "        if not self.clauses:\n",
    "            return \"\", []\n",
    "\n",
    "        where_conditions = [] \n",
    "\n",
    "        for clause in self.clauses:\n",
    "            if isinstance(clause, Predicates):\n",
    "                child_where_clause, params = clause.build_query(params)\n",
    "                where_conditions.append(f\"({child_where_clause})\")\n",
    "            elif isinstance(clause, tuple):\n",
    "                if len(clause) == 2:\n",
    "                    field, value = clause\n",
    "                    operator = \"=\"  # Default operator\n",
    "                elif len(clause) == 3:\n",
    "                    field, operator, value = clause\n",
    "                    if operator not in self.operators_mapping:\n",
    "                       raise ValueError(f\"Invalid operator: {operator}\") \n",
    "                    operator = self.operators_mapping[operator]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid clause format\")\n",
    "            \n",
    "                field_cast = ''\n",
    "                if isinstance(value, int):\n",
    "                    field_cast = '::int'\n",
    "                elif isinstance(value, float):\n",
    "                    field_cast = '::numeric'  \n",
    "\n",
    "                index = len(params)+1\n",
    "                param_name = f\"${index}\"\n",
    "                where_conditions.append(f\"(metadata->>'{field}'){field_cast} {operator} {param_name}\")\n",
    "                params.append(value) \n",
    "\n",
    "        if self.operator == 'NOT':\n",
    "            or_clauses = (\" OR \").join(where_conditions)\n",
    "            #use IS DISTINCT FROM to treat all-null clauses as False and pass the filter\n",
    "            where_clause = f\"TRUE IS DISTINCT FROM ({or_clauses})\"\n",
    "        else:\n",
    "            where_clause = (\" \"+self.operator+\" \").join(where_conditions)\n",
    "        return where_clause, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class QueryBuilder:\n",
    "    def __init__(\n",
    "            self,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str,\n",
    "            id_type: str,\n",
    "            time_partition_interval: Optional[timedelta]) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a base Vector object to generate queries for vector clients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.table_name = table_name\n",
    "        self.num_dimensions = num_dimensions\n",
    "        if distance_type == 'cosine' or distance_type == '<=>':\n",
    "            self.distance_type = '<=>'\n",
    "        elif distance_type == 'euclidean' or distance_type == '<->' or distance_type == 'l2':\n",
    "            self.distance_type = '<->'\n",
    "        else:\n",
    "            raise ValueError(f\"unrecognized distance_type {distance_type}\")\n",
    "\n",
    "        if id_type.lower() != 'uuid' and id_type.lower() != 'text':\n",
    "            raise ValueError(f\"unrecognized id_type {id_type}\")\n",
    "\n",
    "        if time_partition_interval is not None and id_type.lower() != 'uuid':\n",
    "            raise ValueError(f\"time partitioning is only supported for uuid id_type\")\n",
    "\n",
    "        self.id_type = id_type.lower()\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "\n",
    "    def _quote_ident(self, ident):\n",
    "        \"\"\"\n",
    "        Quotes an identifier to prevent SQL injection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ident\n",
    "            The identifier to be quoted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The quoted identifier.\n",
    "        \"\"\"\n",
    "        return '\"{}\"'.format(ident.replace('\"', '\"\"'))\n",
    "\n",
    "    def get_row_exists_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to check if any rows exist in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The query to check for row existence.\n",
    "        \"\"\"\n",
    "        return \"SELECT 1 FROM {table_name} LIMIT 1\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def get_upsert_query(self):\n",
    "        \"\"\"\n",
    "        Generates an upsert query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The upsert query.\n",
    "        \"\"\"\n",
    "        return \"INSERT INTO {table_name} (id, metadata, contents, embedding) VALUES ($1, $2, $3, $4) ON CONFLICT DO NOTHING\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def get_approx_count_query(self):\n",
    "        \"\"\"\n",
    "        Generate a query to find the approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: the query.\n",
    "        \"\"\"\n",
    "        # todo optimize with approx\n",
    "        return \"SELECT COUNT(*) as cnt FROM {table_name}\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    #| export\n",
    "    def get_create_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to create the tables, indexes, and extensions needed to store the vector data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The create table query.\n",
    "        \"\"\"\n",
    "        hypertable_sql = \"\"\n",
    "        if self.time_partition_interval is not None:\n",
    "            hypertable_sql = '''\n",
    "                CREATE EXTENSION IF NOT EXISTS timescaledb;\n",
    "\n",
    "                CREATE OR REPLACE FUNCTION public.uuid_timestamp(uuid UUID) RETURNS TIMESTAMPTZ AS $$\n",
    "                DECLARE\n",
    "                bytes bytea;\n",
    "                BEGIN\n",
    "                bytes := uuid_send(uuid);\n",
    "                if  (get_byte(bytes, 6) >> 4)::int2 != 1 then\n",
    "                    RAISE EXCEPTION 'UUID version is not 1';\n",
    "                end if;\n",
    "                RETURN to_timestamp(\n",
    "                            (\n",
    "                                (\n",
    "                                (get_byte(bytes, 0)::bigint << 24) |\n",
    "                                (get_byte(bytes, 1)::bigint << 16) |\n",
    "                                (get_byte(bytes, 2)::bigint <<  8) |\n",
    "                                (get_byte(bytes, 3)::bigint <<  0)\n",
    "                                ) + (\n",
    "                                ((get_byte(bytes, 4)::bigint << 8 |\n",
    "                                get_byte(bytes, 5)::bigint)) << 32\n",
    "                                ) + (\n",
    "                                (((get_byte(bytes, 6)::bigint & 15) << 8 | get_byte(bytes, 7)::bigint) & 4095) << 48\n",
    "                                ) - 122192928000000000\n",
    "                            ) / 10000 / 1000::double precision\n",
    "                        );\n",
    "                END\n",
    "                $$ LANGUAGE plpgsql\n",
    "                IMMUTABLE PARALLEL SAFE\n",
    "                RETURNS NULL ON NULL INPUT;\n",
    "\n",
    "                SELECT create_hypertable('{table_name}', 'id', time_partitioning_func=>'public.uuid_timestamp', chunk_time_interval => '{chunk_time_interval} seconds'::interval);\n",
    "            '''.format(\n",
    "                table_name=self._quote_ident(self.table_name), \n",
    "                chunk_time_interval=str(self.time_partition_interval.total_seconds()),\n",
    "                )\n",
    "        return '''\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    id {id_type} PRIMARY KEY,\n",
    "    metadata JSONB,\n",
    "    contents TEXT,\n",
    "    embedding VECTOR({dimensions})\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS {index_name} ON {table_name} USING GIN(metadata jsonb_path_ops);\n",
    "\n",
    "{hypertable_sql}\n",
    "'''.format(\n",
    "            table_name=self._quote_ident(self.table_name), \n",
    "            id_type=self.id_type, \n",
    "            index_name=self._quote_ident(self.table_name+\"_meta_idx\"), \n",
    "            dimensions=self.num_dimensions,\n",
    "            hypertable_sql=hypertable_sql,\n",
    "            )\n",
    "\n",
    "    def _get_embedding_index_name(self):\n",
    "        return self._quote_ident(self.table_name+\"_embedding_idx\")\n",
    "\n",
    "    def drop_embedding_index_query(self):\n",
    "        return \"DROP INDEX IF EXISTS {index_name};\".format(index_name=self._get_embedding_index_name())\n",
    "\n",
    "    def delete_all_query(self):\n",
    "        return \"TRUNCATE {table_name};\".format(table_name=self._quote_ident(self.table_name))\n",
    "\n",
    "    def delete_by_ids_query(self, ids: Union[List[uuid.UUID], List[str]]) -> Tuple[str, List]:\n",
    "        query = \"DELETE FROM {table_name} WHERE id = ANY($1::{id_type}[]);\".format(\n",
    "            table_name=self._quote_ident(self.table_name), id_type=self.id_type)\n",
    "        return (query, [ids])\n",
    "\n",
    "    def delete_by_metadata_query(self, filter: Union[Dict[str, str], List[Dict[str, str]]]) -> Tuple[str, List]:\n",
    "        params: List[Any] = []\n",
    "        (where, params) = self._where_clause_for_filter(params, filter)\n",
    "        query = \"DELETE FROM {table_name} WHERE {where};\".format(\n",
    "            table_name=self._quote_ident(self.table_name), where=where)\n",
    "        return (query, params)\n",
    "\n",
    "    def drop_table_query(self):\n",
    "        return \"DROP TABLE IF EXISTS {table_name};\".format(table_name=self._quote_ident(self.table_name))\n",
    "    \n",
    "    def default_max_db_connection_query(self):\n",
    "        \"\"\"\n",
    "        Generates a query to get the default max db connections. This uses a heuristic to determine the max connections based on the max_connections setting in postgres\n",
    "        and the number of currently used connections. This heuristic leaves 4 connections in reserve.\n",
    "        \"\"\"\n",
    "        return \"SELECT greatest(1, ((SELECT setting::int FROM pg_settings WHERE name='max_connections')-(SELECT count(*) FROM pg_stat_activity) - 4)::int)\"\n",
    "\n",
    "    def create_ivfflat_index_query(self, num_records):\n",
    "        \"\"\"\n",
    "        Generates an ivfflat index creation query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_records\n",
    "            The number of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            str: The index creation query.\n",
    "        \"\"\"\n",
    "        column_name = \"embedding\"\n",
    "\n",
    "        index_method = \"invalid\"\n",
    "        if self.distance_type == \"<->\":\n",
    "            index_method = \"vector_l2_ops\"\n",
    "        elif self.distance_type == \"<#>\":\n",
    "            index_method = \"vector_ip_ops\"\n",
    "        elif self.distance_type == \"<=>\":\n",
    "            index_method = \"vector_cosine_ops\"\n",
    "        else:\n",
    "            raise ValueError(f\"unrecognized operator {query_operator}\")\n",
    "\n",
    "        num_lists = num_records / 1000\n",
    "        if num_lists < 10:\n",
    "            num_lists = 10\n",
    "        if num_records > 1000000:\n",
    "            num_lists = math.sqrt(num_records)\n",
    "\n",
    "        return \"CREATE INDEX {index_name} ON {table_name} USING ivfflat ({column_name} {index_method}) WITH (lists = {num_lists});\"\\\n",
    "            .format(index_name=self._get_embedding_index_name(), table_name=self._quote_ident(self.table_name), column_name=self._quote_ident(column_name), index_method=index_method, num_lists=num_lists)\n",
    "\n",
    "    def _where_clause_for_filter(self, params: List, filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]]) -> Tuple[str, List]:\n",
    "        if filter == None:\n",
    "            return (\"TRUE\", params)\n",
    "\n",
    "        if isinstance(filter, dict):\n",
    "            where = \"metadata @> ${index}\".format(index=len(params)+1)\n",
    "            json_object = json.dumps(filter)\n",
    "            params = params + [json_object]\n",
    "        elif isinstance(filter, list):\n",
    "            any_params = []\n",
    "            for idx, filter_dict in enumerate(filter, start=len(params) + 1):\n",
    "                any_params.append(json.dumps(filter_dict))\n",
    "            where = \"metadata @> ANY(${index}::jsonb[])\".format(\n",
    "                index=len(params) + 1)\n",
    "            params = params + [any_params]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown filter type: {filter_type}\".format(filter_type=type(filter)))\n",
    "\n",
    "        return (where, params)\n",
    "\n",
    "    def search_query(self, query_embedding: Optional[Union[List[float], np.ndarray]], limit: int = 10, filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None, predicates: Optional[Predicates] = None) -> Tuple[str, List]:\n",
    "        \"\"\"\n",
    "        Generates a similarity query.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List]: A tuple containing the query and parameters.\n",
    "        \"\"\"\n",
    "        params: List[Any] = []\n",
    "        if query_embedding is not None:\n",
    "            distance = \"embedding {op} ${index}\".format(\n",
    "                op=self.distance_type, index=len(params)+1)\n",
    "            params = params + [query_embedding]\n",
    "            order_by_clause = \"ORDER BY {distance} ASC\".format(\n",
    "                distance=distance)\n",
    "        else:\n",
    "            distance = \"-1.0\"\n",
    "            order_by_clause = \"\"\n",
    "\n",
    "        where_clauses = []\n",
    "        if filter is not None:\n",
    "            (where_filter, params) = self._where_clause_for_filter(params, filter)\n",
    "            where_clauses.append(where_filter)\n",
    "\n",
    "        if predicates is not None:\n",
    "            (where_predicates, params) = predicates.build_query(params)\n",
    "            where_clauses.append(where_predicates)\n",
    "        \n",
    "        if len(where_clauses) > 0:\n",
    "            where = \" AND \".join(where_clauses)\n",
    "        else:\n",
    "            where = \"TRUE\"\n",
    "\n",
    "        query = '''\n",
    "        SELECT\n",
    "            id, metadata, contents, embedding, {distance} as distance\n",
    "        FROM\n",
    "           {table_name}\n",
    "        WHERE \n",
    "           {where}\n",
    "        {order_by_clause}\n",
    "        LIMIT {limit}\n",
    "        '''.format(distance=distance, order_by_clause=order_by_clause, where=where, table_name=self._quote_ident(self.table_name), limit=limit)\n",
    "        return (query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryBuilder.get_create_query\n",
       "\n",
       ">      QueryBuilder.get_create_query ()\n",
       "\n",
       "Generates a query to create the tables, indexes, and extensions needed to store the vector data."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryBuilder.get_create_query\n",
       "\n",
       ">      QueryBuilder.get_create_query ()\n",
       "\n",
       "Generates a query to create the tables, indexes, and extensions needed to store the vector data."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(QueryBuilder.get_create_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Async(QueryBuilder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            service_url: str,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str = 'cosine',\n",
    "            id_type='UUID',\n",
    "            time_partition_interval: Optional[timedelta] = None,\n",
    "            max_db_connections: Optional[int] = None\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a async client for storing vector data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        service_url\n",
    "            The connection string for the database.\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.builder = QueryBuilder(\n",
    "            table_name, num_dimensions, distance_type, id_type, time_partition_interval)\n",
    "        self.service_url = service_url\n",
    "        self.pool = None\n",
    "        self.max_db_connections = max_db_connections\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "\n",
    "    async def _default_max_db_connections(self) -> int:\n",
    "        \"\"\"\n",
    "        Gets a default value for the number of max db connections to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.default_max_db_connection_query()\n",
    "        conn = await asyncpg.connect(dsn=self.service_url)\n",
    "        num_connections = await conn.fetchval(query)\n",
    "        await conn.close()\n",
    "        return num_connections\n",
    "\n",
    "\n",
    "    async def connect(self):\n",
    "        \"\"\"\n",
    "        Establishes a connection to a PostgreSQL database using asyncpg.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            asyncpg.Connection: The established database connection.\n",
    "        \"\"\"\n",
    "        if self.pool == None:\n",
    "            if self.max_db_connections == None:\n",
    "                self.max_db_connections = await self._default_max_db_connections()\n",
    "            async def init(conn):\n",
    "                await register_vector(conn)\n",
    "                # decode to a dict, but accept a string as input in upsert\n",
    "                await conn.set_type_codec(\n",
    "                    'jsonb',\n",
    "                    encoder=str,\n",
    "                    decoder=json.loads,\n",
    "                    schema='pg_catalog')\n",
    "\n",
    "            self.pool = await asyncpg.create_pool(dsn=self.service_url, init=init, min_size=1, max_size=self.max_db_connections)\n",
    "        return self.pool.acquire()\n",
    "\n",
    "    async def close(self):\n",
    "        if self.pool != None:\n",
    "            await self.pool.close()\n",
    "\n",
    "    async def table_is_empty(self):\n",
    "        \"\"\"\n",
    "        Checks if the table is empty.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            bool: True if the table is empty, False otherwise.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_row_exists_query()\n",
    "        async with await self.connect() as pool:\n",
    "            rec = await pool.fetchrow(query)\n",
    "            return rec == None\n",
    "\n",
    "    def munge_record(self, records) -> Iterable[Tuple[uuid.UUID, str, str, List[float]]]:\n",
    "        metadata_is_dict = isinstance(records[0][1], dict)\n",
    "        if metadata_is_dict:\n",
    "           records = map(lambda item: Async._convert_record_meta_to_json(item), records)\n",
    "\n",
    "        return records \n",
    "\n",
    "    def _convert_record_meta_to_json(item):\n",
    "        if not isinstance(item[1], dict):\n",
    "            raise ValueError(\n",
    "                \"Cannot mix dictionary and string metadata fields in the same upsert\")\n",
    "        return (item[0], json.dumps(item[1]), item[2], item[3])\n",
    "\n",
    "    async def upsert(self, records):\n",
    "        \"\"\"\n",
    "        Performs upsert operation for multiple records.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        records\n",
    "            List of records to upsert. Each record is a tuple of the form (id, metadata, contents, embedding).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        records = self.munge_record(records)\n",
    "        query = self.builder.get_upsert_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.executemany(query, records)\n",
    "\n",
    "    async def create_tables(self):\n",
    "        \"\"\"\n",
    "        Creates necessary tables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.get_create_query()\n",
    "        #don't use a connection pool for this because the vector extension may not be installed yet and if it's not installed, register_vector will fail.\n",
    "        conn = await asyncpg.connect(dsn=self.service_url)\n",
    "        await conn.execute(query)\n",
    "        await conn.close()\n",
    "\n",
    "    async def delete_all(self, drop_index=True):\n",
    "        \"\"\"\n",
    "        Deletes all data. Also drops the index if `drop_index` is true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if drop_index:\n",
    "            await self.drop_embedding_index()\n",
    "        query = self.builder.delete_all_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def delete_by_ids(self, ids: Union[List[uuid.UUID], List[str]]):\n",
    "        \"\"\"\n",
    "        Delete records by id.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_ids_query(ids)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)\n",
    "\n",
    "    async def delete_by_metadata(self, filter: Union[Dict[str, str], List[Dict[str, str]]]):\n",
    "        \"\"\"\n",
    "        Delete records by metadata filters.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_metadata_query(filter)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)\n",
    "\n",
    "    async def drop_table(self):\n",
    "        \"\"\"\n",
    "        Drops the table\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_table_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def _get_approx_count(self):\n",
    "        \"\"\"\n",
    "        Retrieves an approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: Approximate count of records.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_approx_count_query()\n",
    "        async with await self.connect() as pool:\n",
    "            rec = await pool.fetchrow(query)\n",
    "            return rec[0]\n",
    "\n",
    "    async def drop_embedding_index(self):\n",
    "        \"\"\"\n",
    "        Drop any index on the emedding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_embedding_index_query()\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def create_ivfflat_index(self, num_records=None):\n",
    "        \"\"\"\n",
    "        Creates an ivfflat index for the table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_records : int, optional\n",
    "            The number of records. If None, it's calculated. Default is None.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if num_records == None:\n",
    "            num_records = await self._get_approx_count()\n",
    "        query = self.builder.create_ivfflat_index_query(num_records)\n",
    "        async with await self.connect() as pool:\n",
    "            await pool.execute(query)\n",
    "\n",
    "    async def search(self,\n",
    "                     query_embedding: Optional[List[float]] = None, \n",
    "                     limit: int = 10,\n",
    "                     filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n",
    "                     predicates: Optional[Predicates] = None): \n",
    "        \"\"\"\n",
    "        Retrieves similar records using a similarity query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_embedding \n",
    "            The query embedding vector.\n",
    "        limit \n",
    "            The number of nearest neighbors to retrieve.\n",
    "        filter \n",
    "            A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched).\n",
    "        predicates\n",
    "            A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            List: List of similar records.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.search_query(\n",
    "            query_embedding, limit, filter, predicates)\n",
    "        async with await self.connect() as pool:\n",
    "            return await pool.fetch(query, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.create_tables\n",
       "\n",
       ">      Async.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L633){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.search\n",
       "\n",
       ">      Async.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                    filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=No\n",
       ">                    ne, predicates:Optional[__main__.Predicates]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L633){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Async.search\n",
       "\n",
       ">      Async.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                    filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=No\n",
       ">                    ne, predicates:Optional[__main__.Predicates]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Async.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "con = await asyncpg.connect(service_url)\n",
    "await con.execute(\"DROP TABLE IF EXISTS data_table;\")\n",
    "await con.execute(\"DROP EXTENSION IF EXISTS vector CASCADE;\")\n",
    "await con.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Async(service_url, \"data_table\", 2)\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "\n",
    "await vec.upsert([\n",
    "    (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n",
    "    (uuid.uuid4(), '''{\"key\":\"val2\", \"key_10\": \"10\", \"key_11\": \"11.3\"}''', \"the brown fox\", [1.0, 1.4]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n",
    "     \"the brown fox\", [1.0, 1.8]),\n",
    "])\n",
    "\n",
    "await vec.create_ivfflat_index()\n",
    "await vec.drop_embedding_index()\n",
    "await vec.create_ivfflat_index(100)\n",
    "\n",
    "rec = await vec.search([1.0, 2.0])\n",
    "assert len(rec) == 10\n",
    "rec = await vec.search([1.0, 2.0], limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = await vec.search(limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search(limit=4, filter={\"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "rec = await vec.search(limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\")))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"==\", \"val2\")))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<\", 100)))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<\", 10)))\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<=\", 10)))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<=\", 10.0)))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_11\", \"<=\", 11.3)))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search(limit=4, predicates=Predicates((\"key_11\", \">=\", 11.29999)))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_11\", \"<\", 11.299999)))\n",
    "assert len(rec) == 0\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates(*[(\"key\", \"val2\"), (\"key_10\", \"<\", 100)]))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_10\", \"<\", 100), operator='AND'))\n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\"), (\"key_2\", \"val_2\"), operator='OR'))\n",
    "assert len(rec) == 2\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<\", 100)) & (Predicates((\"key\", \"val2\")) | Predicates((\"key_2\", \"val_2\")))) \n",
    "assert len(rec) == 1\n",
    "rec = await vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key_10\", \"<\", 100)) and (Predicates((\"key\", \"val2\")) or Predicates((\"key_2\", \"val_2\")))) \n",
    "assert len(rec) == 1\n",
    "rec = await vec.search(limit=4, predicates=~Predicates((\"key\", \"val2\"), (\"key_10\", \"<\", 100)))\n",
    "assert len(rec) == 4\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except ValueError as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries opposite order\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "await vec.delete_by_ids([rec[0][SEARCH_RESULT_ID_IDX]])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 1\n",
    "await vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 0\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 4\n",
    "await vec.delete_by_metadata([{\"key2\": \"val\"}])\n",
    "rec = await vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 0\n",
    "\n",
    "assert not await vec.table_is_empty()\n",
    "await vec.delete_all()\n",
    "assert await vec.table_is_empty()\n",
    "\n",
    "await vec.drop_table()\n",
    "await vec.close()\n",
    "\n",
    "vec = Async(service_url, \"data_table\", 2, id_type=\"TEXT\")\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "await vec.delete_by_ids([\"Not a valid UUID\"])\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "await vec.drop_table()\n",
    "await vec.close()\n",
    "\n",
    "vec = Async(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60))\n",
    "await vec.create_tables()\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "id = uuid.uuid1()\n",
    "await vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = await vec.table_is_empty()\n",
    "assert not empty\n",
    "await vec.delete_by_ids([id])\n",
    "empty = await vec.table_is_empty()\n",
    "assert empty\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert with uuid type 4 in time partitioned table\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "await vec.drop_table()\n",
    "await vec.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import psycopg2.pool\n",
    "from contextlib import contextmanager\n",
    "import psycopg2.extras\n",
    "import pgvector.psycopg2\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sync:\n",
    "    translated_queries: Dict[str, str] = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            service_url: str,\n",
    "            table_name: str,\n",
    "            num_dimensions: int,\n",
    "            distance_type: str = 'cosine',\n",
    "            id_type='UUID',\n",
    "            time_partition_interval: Optional[timedelta] = None,\n",
    "            max_db_connections: Optional[int] = None\n",
    "            ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a sync client for storing vector data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        service_url\n",
    "            The connection string for the database.\n",
    "        table_name\n",
    "            The name of the table.\n",
    "        num_dimensions\n",
    "            The number of dimensions for the embedding vector.\n",
    "        distance_type\n",
    "            The distance type for indexing.\n",
    "        id_type\n",
    "            The type of the primary id column. Can be either 'UUID' or 'TEXT'.\n",
    "        \"\"\"\n",
    "        self.builder = QueryBuilder(\n",
    "            table_name, num_dimensions, distance_type, id_type, time_partition_interval)\n",
    "        self.service_url = service_url\n",
    "        self.pool = None\n",
    "        self.max_db_connections = max_db_connections\n",
    "        self.time_partition_interval = time_partition_interval\n",
    "        psycopg2.extras.register_uuid()\n",
    "\n",
    "    def default_max_db_connections(self):\n",
    "        \"\"\"\n",
    "        Gets a default value for the number of max db connections to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.default_max_db_connection_query()\n",
    "        conn = psycopg2.connect(dsn=self.service_url)\n",
    "        with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                num_connections = cur.fetchone() \n",
    "        conn.close()\n",
    "        return num_connections[0]\n",
    "\n",
    "    @contextmanager\n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        Establishes a connection to a PostgreSQL database using psycopg2 and allows it's\n",
    "        use in a context manager.\n",
    "        \"\"\"\n",
    "        if self.pool == None:\n",
    "            if self.max_db_connections == None:\n",
    "                self.max_db_connections = self.default_max_db_connections()\n",
    "\n",
    "            self.pool = psycopg2.pool.SimpleConnectionPool(\n",
    "                1, self.max_db_connections, dsn=self.service_url)\n",
    "\n",
    "        connection = self.pool.getconn()\n",
    "        pgvector.psycopg2.register_vector(connection)\n",
    "        try:\n",
    "            yield connection\n",
    "            connection.commit()\n",
    "        finally:\n",
    "            self.pool.putconn(connection)\n",
    "\n",
    "    def close(self):\n",
    "        if self.pool != None:\n",
    "            self.pool.closeall()\n",
    "\n",
    "    def _translate_to_pyformat(self, query_string, params):\n",
    "        \"\"\"\n",
    "        Translates dollar sign number parameters and list parameters to pyformat strings.\n",
    "\n",
    "        Args:\n",
    "            query_string (str): The query string with parameters.\n",
    "            params (list): List of parameter values.\n",
    "\n",
    "        Returns:\n",
    "            str: The query string with translated pyformat parameters.\n",
    "            dict: A dictionary mapping parameter numbers to their values.\n",
    "        \"\"\"\n",
    "\n",
    "        translated_params = {}\n",
    "        if params != None:\n",
    "            for idx, param in enumerate(params):\n",
    "                translated_params[str(idx+1)] = param\n",
    "\n",
    "        if query_string in self.translated_queries:\n",
    "            return self.translated_queries[query_string], translated_params\n",
    "\n",
    "        dollar_params = re.findall(r'\\$[0-9]+', query_string)\n",
    "        translated_string = query_string\n",
    "        for dollar_param in dollar_params:\n",
    "            # Extract the number after the $\n",
    "            param_number = int(dollar_param[1:])\n",
    "            if params != None:\n",
    "                pyformat_param = '%s' if param_number == 0 else f'%({param_number})s'\n",
    "            else:\n",
    "                pyformat_param = '%s'\n",
    "            translated_string = translated_string.replace(\n",
    "                dollar_param, pyformat_param)\n",
    "\n",
    "        self.translated_queries[query_string] = translated_string\n",
    "        return self.translated_queries[query_string], translated_params\n",
    "\n",
    "    def table_is_empty(self):\n",
    "        \"\"\"\n",
    "        Checks if the table is empty.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            bool: True if the table is empty, False otherwise.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_row_exists_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                rec = cur.fetchone()\n",
    "                return rec == None\n",
    "    \n",
    "    def munge_record(self, records) -> Iterable[Tuple[uuid.UUID, str, str, List[float]]]:\n",
    "        metadata_is_dict = isinstance(records[0][1], dict)\n",
    "        if metadata_is_dict:\n",
    "           records = map(lambda item: Sync._convert_record_meta_to_json(item), records)\n",
    "\n",
    "        return records\n",
    "\n",
    "\n",
    "    def _convert_record_meta_to_json(item):\n",
    "        if not isinstance(item[1], dict):\n",
    "            raise ValueError(\n",
    "                \"Cannot mix dictionary and string metadata fields in the same upsert\")\n",
    "        return (item[0], json.dumps(item[1]), item[2], item[3])\n",
    "\n",
    "    def upsert(self, records):\n",
    "        \"\"\"\n",
    "        Performs upsert operation for multiple records.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        records\n",
    "            Records to upsert.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        records = self.munge_record(records)\n",
    "        query = self.builder.get_upsert_query()\n",
    "        query, _ = self._translate_to_pyformat(query, None)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.executemany(query, records)\n",
    "\n",
    "    def create_tables(self):\n",
    "        \"\"\"\n",
    "        Creates necessary tables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.get_create_query()\n",
    "        #don't use a connection pool for this because the vector extension may not be installed yet and if it's not installed, register_vector will fail.\n",
    "        conn = psycopg2.connect(dsn=self.service_url)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def delete_all(self, drop_index=True):\n",
    "        \"\"\"\n",
    "        Deletes all data. Also drops the index if `drop_index` is true.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if drop_index:\n",
    "            self.drop_embedding_index()\n",
    "        query = self.builder.delete_all_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def delete_by_ids(self, ids: Union[List[uuid.UUID], List[str]]):\n",
    "        \"\"\"\n",
    "        Delete records by id.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ids\n",
    "            List of ids to delete.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_ids_query(ids)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "\n",
    "    def delete_by_metadata(self, filter: Union[Dict[str, str], List[Dict[str, str]]]):\n",
    "        \"\"\"\n",
    "        Delete records by metadata filters.\n",
    "        \"\"\"\n",
    "        (query, params) = self.builder.delete_by_metadata_query(filter)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "\n",
    "    def drop_table(self):\n",
    "        \"\"\"\n",
    "        Drops the table\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_table_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def _get_approx_count(self):\n",
    "        \"\"\"\n",
    "        Retrieves an approximate count of records in the table.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: Approximate count of records.\n",
    "        \"\"\"\n",
    "        query = self.builder.get_approx_count_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "                rec = cur.fetchone()\n",
    "                return rec[0]\n",
    "\n",
    "    def drop_embedding_index(self):\n",
    "        \"\"\"\n",
    "        Drop any index on the emedding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        query = self.builder.drop_embedding_index_query()\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def create_ivfflat_index(self, num_records=None):\n",
    "        \"\"\"\n",
    "        Creates an ivfflat index for the table.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_records\n",
    "            The number of records. If None, it's calculated. Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            None\n",
    "        \"\"\"\n",
    "        if num_records == None:\n",
    "            num_records = self._get_approx_count()\n",
    "        query = self.builder.create_ivfflat_index_query(num_records)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query)\n",
    "\n",
    "    def search(self, \n",
    "    query_embedding: Optional[List[float]] = None, \n",
    "    limit: int = 10, \n",
    "    filter: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n",
    "    predicates: Optional[Predicates] = None):\n",
    "        \"\"\"\n",
    "        Retrieves similar records using a similarity query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_embedding \n",
    "            The query embedding vector.\n",
    "        limit \n",
    "            The number of nearest neighbors to retrieve.\n",
    "        filter \n",
    "            A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched).\n",
    "        predicates\n",
    "            A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, |, and ~).\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            List: List of similar records.\n",
    "        \"\"\"\n",
    "        if query_embedding is not None:\n",
    "            query_embedding_np = np.array(query_embedding)\n",
    "        else:\n",
    "            query_embedding_np = None\n",
    "\n",
    "        (query, params) = self.builder.search_query(\n",
    "            query_embedding_np, limit, filter, predicates)\n",
    "        query, params = self._translate_to_pyformat(query, params)\n",
    "        with self.connect() as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(query, params)\n",
    "                return cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L827){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.create_tables\n",
       "\n",
       ">      Sync.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L827){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.create_tables\n",
       "\n",
       ">      Sync.create_tables ()\n",
       "\n",
       "Creates necessary tables."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.create_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L804){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.upsert\n",
       "\n",
       ">      Sync.upsert (records)\n",
       "\n",
       "Performs upsert operation for multiple records.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| records |  | Records to upsert. |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L804){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.upsert\n",
       "\n",
       ">      Sync.upsert (records)\n",
       "\n",
       "Performs upsert operation for multiple records.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| records |  | Records to upsert. |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L944){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.search\n",
       "\n",
       ">      Sync.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                   filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=Non\n",
       ">                   e, predicates:Optional[__main__.Predicates]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/timescale/python-vector/blob/main/timescale_vector/client.py#L944){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sync.search\n",
       "\n",
       ">      Sync.search (query_embedding:Optional[List[float]]=None, limit:int=10,\n",
       ">                   filter:Union[Dict[str,str],List[Dict[str,str]],NoneType]=Non\n",
       ">                   e, predicates:Optional[__main__.Predicates]=None)\n",
       "\n",
       "Retrieves similar records using a similarity query.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| query_embedding | Optional | None | The query embedding vector. |\n",
       "| limit | int | 10 | The number of nearest neighbors to retrieve. |\n",
       "| filter | Union | None | A filter for metadata. Should be specified as a key-value object or a list of key-value objects (where any objects in the list are matched). |\n",
       "| predicates | Optional | None | A Predicates object to filter the results. Predicates support more complex queries than the filter parameter. Predicates can be combined using logical operators (&, \\|, and ~). |\n",
       "| **Returns** | **List: List of similar records.** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sync.search)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "con = await asyncpg.connect(service_url)\n",
    "await con.execute(\"DROP TABLE IF EXISTS data_table;\")\n",
    "await con.execute(\"DROP EXTENSION IF EXISTS vector CASCADE\")\n",
    "await con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Sync(service_url, \"data_table\", 2)\n",
    "vec.create_tables()\n",
    "empty = vec.table_is_empty()\n",
    "\n",
    "assert empty\n",
    "vec.upsert([(uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "empty = vec.table_is_empty()\n",
    "assert not empty\n",
    "\n",
    "vec.upsert([\n",
    "    (uuid.uuid4(), '''{\"key\":\"val\"}''', \"the brown fox\", [1.0, 1.3]),\n",
    "    (uuid.uuid4(), '''{\"key\":\"val2\"}''', \"the brown fox\", [1.0, 1.4]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.5]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.6]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val2\"}''', \"the brown fox\", [1.0, 1.7]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.9]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 100.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 101.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.8]),\n",
    "    (uuid.uuid4(), '''{\"key_1\":\"val_1\", \"key_2\":\"val_2\"}''',\n",
    "     \"the brown fox\", [1.0, 1.8]),\n",
    "])\n",
    "\n",
    "vec.create_ivfflat_index()\n",
    "vec.drop_embedding_index()\n",
    "vec.create_ivfflat_index(10)\n",
    "\n",
    "rec = vec.search([1.0, 2.0])\n",
    "assert len(rec) == 10\n",
    "rec = vec.search(np.array([1.0, 2.0]))\n",
    "assert len(rec) == 10\n",
    "rec = vec.search([1.0, 2.0], limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = vec.search(limit=4)\n",
    "assert len(rec) == 4\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"val2\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = vec.search(limit=4, filter={\"key2\": \"does not exist\"})\n",
    "assert len(rec) == 0\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\"key_1\": \"val_1\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert len(rec) == 1\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter={\n",
    "                 \"key_1\": \"val_1\", \"key_2\": \"val_3\"})\n",
    "assert len(rec) == 0\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key_1\": \"val_1\"}, {\n",
    "                 \"key2\": \"val2\"}, {\"no such key\": \"no such val\"}])\n",
    "assert len(rec) == 2\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except ValueError as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert using both keys and dictionaries opposite order\n",
    "    await vec.upsert([\n",
    "        (uuid.uuid4(), '''{\"key2\":\"val\"}''', \"the brown fox\", [1.0, 1.2]),\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "\n",
    "rec = vec.search([1.0, 2.0], filter={\"key_1\": \"val_1\", \"key_2\": \"val_2\"})\n",
    "assert rec[0][SEARCH_RESULT_CONTENTS_IDX] == 'the brown fox'\n",
    "assert rec[0][SEARCH_RESULT_METADATA_IDX] == {\n",
    "    'key_1': 'val_1', 'key_2': 'val_2'}\n",
    "assert isinstance(rec[0][SEARCH_RESULT_METADATA_IDX], dict)\n",
    "assert rec[0][SEARCH_RESULT_DISTANCE_IDX] == 0.0009438353921149556\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, predicates=Predicates((\"key\", \"val2\")))\n",
    "assert len(rec) == 1\n",
    "\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "len(rec) == 2\n",
    "vec.delete_by_ids([rec[0][SEARCH_RESULT_ID_IDX]])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 1\n",
    "vec.delete_by_metadata([{\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[\n",
    "                 {\"key_1\": \"val_1\"}, {\"key2\": \"val2\"}])\n",
    "assert len(rec) == 0\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "assert len(rec) == 4\n",
    "vec.delete_by_metadata([{\"key2\": \"val\"}])\n",
    "rec = vec.search([1.0, 2.0], limit=4, filter=[{\"key2\": \"val\"}])\n",
    "len(rec) == 0\n",
    "\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_all()\n",
    "assert vec.table_is_empty()\n",
    "\n",
    "vec.drop_table()\n",
    "vec.close()\n",
    "\n",
    "vec = Sync(service_url, \"data_table\", 2, id_type=\"TEXT\")\n",
    "vec.create_tables()\n",
    "assert vec.table_is_empty()\n",
    "vec.upsert([(\"Not a valid UUID\", {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_by_ids([\"Not a valid UUID\"])\n",
    "assert vec.table_is_empty()\n",
    "vec.drop_table()\n",
    "vec.close()\n",
    "\n",
    "vec = Sync(service_url, \"data_table\", 2, time_partition_interval=timedelta(seconds=60))\n",
    "vec.create_tables()\n",
    "assert vec.table_is_empty()\n",
    "id = uuid.uuid1()\n",
    "vec.upsert([(id, {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])])\n",
    "assert not vec.table_is_empty()\n",
    "vec.delete_by_ids([id])\n",
    "assert vec.table_is_empty()\n",
    "raised = False\n",
    "try:\n",
    "    # can't upsert with uuid type 4 in time partitioned table\n",
    "    vec.upsert([\n",
    "        (uuid.uuid4(),  {\"key\": \"val\"}, \"the brown fox\", [1.0, 1.2])\n",
    "    ])\n",
    "    #pass\n",
    "except BaseException as e:\n",
    "    raised = True\n",
    "assert raised\n",
    "vec.drop_table()\n",
    "vec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
